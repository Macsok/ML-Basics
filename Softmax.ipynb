{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{\\textcolor{grey}{Author Maciej S., source Wiki}}$\n",
    "## SOFTMAX\n",
    "The $\\textbf{\\textcolor{olive}{softmax function}}$, also known as $\\textbf{\\textcolor{olive}{softargmax}}$ or $\\textbf{\\textcolor{olive}{normalized exponential function}}$, converts vector of $K$ real numbers into a probability distribution of $K$ possible outcomes. It is a generalized of the logistic function to multiply dimensions, and used in multinomial logistic regression. The softmax function is often used as the last $\\textbf{\\textcolor{olive}{activation function}}$ of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "# Acitvation function\n",
    "The $\\textbf{\\textcolor{olive}{activation function}}$ of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the activation function is nonlinear.\n",
    "\n",
    "# Definition\n",
    "Formally, the standard (unit) softmax function $\\sigma:\\R^K \\mapsto (0, 1)^K$, where $K \\geqslant 1$ is defined by $$\\sigma(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\text{\\quad for } i = 1, ..., K \\text{\\quad and } z = (z_1, ..., z_K) \\in \\R^K.$$\n",
    "\n",
    "In words, the softmax applies the standard exponential function to each element $z_i$ of the input vector $z$ (consisting of $K$ real numbers), and normilizes these values by dividing by the sum of all these exponentials. The normalization ensures that the sum of the components of the output vector $\\sigma (z)$ is 1. The term \"softmax\" derives from amplifying effects of the exponential on any maxima in the input vector. For example, the standard softmax of (1, 2, 8) is approximately (0.001, 0.002, 0.997), which amounts to assigning almost all of the total unit weight in the result to the position of the vector's maximal element (8).\n",
    "\n",
    "In general, instead of $e$ a different base $b > 0$ can be used. If $0 < b < 1$, smaller input components will result in larger output probabilities, and decreasing the value of $b$ will create probability distributions that are more concentrated around the positions of the smallest input values. Conversely, as above if $b > 1$ larger input components will result in larger output probabilities, and increasing the value of $b$ will create probability distributions that are more concentrated around the positions of the largest input values. Writing $b=e^\\beta$ yields the expression:\n",
    "$$\\sigma(z)_i = \\frac{e^{\\beta z_i}}{\\sum_{j=1}^K e^{\\beta z_j}}$$\n",
    "\n",
    "The reciprocal of $\\beta$ is sometimes reffered to as the $\\textbf{\\textcolor{olive}{temperature}}$, $T = 1 / \\beta$, with $b = e^{1/T}$. A higher temperature results in a more uniform output distribution (i.e. with higher entropy, and \"more random\"), while a lower temperature results in a sharper output distribution, with one value dominating.\n",
    "\n",
    "In some fileds, the base is fixed, corresponding to a fixed scale, while in others the parameter $\\beta$ is varied.\n",
    "\n",
    "# $\\text{\\textcolor{olive}{In simple words}}$\n",
    "We raise $e$ to the power of an element (which can be negative) to get positive value. Then we divide each element by the sum of all elements, in other words we calculate how significant role does every element play in this sum.\n",
    "\n",
    "# Mathematical property\n",
    "Adding $c = (c, ..., c)$ to the inputs $z$ yields $\\sigma (z + c) = \\sigma$, because it multiplies each exponent by the same factor, $e^c$ (because $e^{z_i+c} = e^{z_i} + e^c$), so the ratio does not change:\n",
    "$$\\sigma(z+c)_i = \\frac{e^{z_i + c}}{\\sum_{k=1}^K e^{z_k + c}} = \\frac{e^{z_i} \\cdot e^c}{\\sum_{k=1}^K e^{z_k} \\cdot e^c} = \\sigma(z)_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0009088005553630333, 0.002470376035336822, 0.9966208234093001]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(args, T, base=np.e):\n",
    "    \"\"\"args - input array, T - temperature, base - base element\"\"\"\n",
    "    #   sum all e^{z*1/T}\n",
    "    s = np.sum([np.power(base, z * 1/T) for z in args])\n",
    "    #   return array of: e^z_i / sum\n",
    "    return [np.power(base, z * 1/T) / s for z in args]\n",
    "\n",
    "softmax([1, 2, 8], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This output proves the example correctness shown in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56107b3e64234cf485179a7ed5a20574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='T', max=10.0, min=0.1), FloatSlider(value=2.50000000â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.printing(T, base)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def printing(T, base):\n",
    "    #   random vals to operate on\n",
    "    values = [-0.8, -5.0, 0.5, 1.5, 3.4, -2.3, 2.5]\n",
    "    soft_vals = softmax(values, T, base)\n",
    "\n",
    "    #create table to display\n",
    "    table = [[values[i], soft_vals[i]] for i in range(len(values))]\n",
    "    print(tabulate(table, ['input val.', 'softed value'], tablefmt='grid'))\n",
    "\n",
    "    plt.bar(values, soft_vals)\n",
    "    plt.show()\n",
    "\n",
    "interact(printing, T=(0.1, 10, 0.1), base=(0.1, 5, 0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
